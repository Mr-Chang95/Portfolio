{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aecac884",
   "metadata": {},
   "source": [
    "# Wrangle Report(WeRateDogs Project)\n",
    "\n",
    "## By Daniel Chang\n",
    "\n",
    "**Date: 12/13/21**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5ca106",
   "metadata": {},
   "source": [
    "In my first data wrangling project, I demonstrated my skills and knowledge by gathering the data(from Twitter), assessing the dataset and finally cleaning it. Wrangling the data provided by WeRateDogs turned out to time-consuming and quite challenging than I had initially thought. It's easy to see how time-consuming it can be since I have spotted more issues that can be addressed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acf09a7",
   "metadata": {},
   "source": [
    "## Gathering\n",
    "First, I downloaded the unclean twitter_archives from the Udacity website manually since it was readily available. Afterward, I had to programmatically download the image_predictions tsv file, using the request library and the url that was provided. I accessed all the file content by using the `get` function on the url then wrote those content onto an empty file.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82ce789",
   "metadata": {},
   "source": [
    "After obtaining those two files, I needed to gather and write the data for a third file which contains the WeRateDogs tweet data which requires authorization for use. set up an application and link it to my twitter account to get unique keys which I could in turn use to create an API object. Therefore, I set up my application to link with my twitter account to get unique keys which can be used to create an API object. We were able to do this with `tweepy`, an access library for the twitter API. \n",
    "\n",
    "Next, we used the `for` loop and `get_status` function to target each 'tweet_id' and convert these content to the json format which is the tweet_json file. Every tweet's json object in the text file was read one by one into an empty list. Lastly, I extracted the favourite and retweet counts for each and every tweet and used their corresponding tweet ids to create a new pandas dataframe.\n",
    "\n",
    "**Note: Since I have been rejected for use by Twitter, I downloaded the tweet_json file from the Udacity website.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149e36db",
   "metadata": {},
   "source": [
    "## Assessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0524f07",
   "metadata": {},
   "source": [
    "After the gathering process, I had to assess for quality and tidiness issues which is step 2. I have found that most of the issuses are in the twitter_archive table which should come as no surpise since its the largest. The `head`,`tail` and `sample` functions are frequently used to visually assess each dataset. Others required a little more analysis, mainly through summaries or filtering out certain sections of the data. While prgrammatically assessing the dataset, the `info`, `describe` and `value_counts` functions were frequently used for this step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1759c18a",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f391d25",
   "metadata": {},
   "source": [
    "The final step of data wrangling is cleaning where I used the used the define, code and test format for each operations. There were many functions being used for this step to fix the following problems: \n",
    "### Quality issues\n",
    "\n",
    "**twitter_archive Table**\n",
    "\n",
    "1.  Retweets are present in a number of rows which we do not want.\n",
    "\n",
    "\n",
    "2.  Inaccurate datatype for **timestamp, tweet_id, rating_numerator and rating_denominator**.\n",
    "\n",
    "\n",
    "3.  The **source** column is a little difficult to read because of the url.\n",
    "\n",
    "\n",
    "4.  Missing values in the following columns: **in_reply_to_status, in_reply_to_user_id, retweeted_status_id, expanded_urls, retweeted_status_user_id, and retweet_status_timestamp**.(Most columns are unneccessary)\n",
    "\n",
    "\n",
    "5.  NaN is represented by the string, 'None'. \n",
    "\n",
    "\n",
    "6.  Date and time values share the same column which makes it untidy.\n",
    "\n",
    "\n",
    "7.  Add **favorite_count and retweet** column from tweet_json table to this one.\n",
    "\n",
    "\n",
    "8.  Incorrect rating_numerator and rating_denominator.\n",
    "**image_predictions Table**\n",
    "\n",
    "\n",
    "9.  Incorrect datatype for **tweet_id**.\n",
    "\n",
    "\n",
    "10.  Underscores in the **p1, p2 and p3** columns should be spaces. \n",
    "\n",
    "\n",
    "11.  **p1, p2 and p3** columns contain inconsistent letter casings. \n",
    "\n",
    "\n",
    "12. **p2, p2_conf, p2_dog, p3, p3_conf, p3_dog** columns are not needed.\n",
    "\n",
    "**tweet_json Table**\n",
    "\n",
    "13. **tweet_id** should be a string.\n",
    "\n",
    "\n",
    "\n",
    "### Tidiness issues\n",
    "\n",
    "**twitter_archive Table**\n",
    "\n",
    "1.  The **doggo, floofer, pupper and puppo** columns should be column values rather than the headers.  \n",
    "\n",
    "**image_predictions Table**\n",
    "\n",
    "2.  Join this table with the twitter_archive table after."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9027f903",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8208f575",
   "metadata": {},
   "source": [
    "With so much unclean data around the world, data wrangling is a skill that is growing in importance and every data analyst should be familir with it. After the wrangling process, we can finally move to the last step where we will use the results to conduct analysis and create visualization for better insights about the data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
